{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063fce38",
   "metadata": {},
   "source": [
    "# CREATING THE SOAM\n",
    "This notebook outlines the approach I used in building up a final SOAM data structure using the Soam Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6492436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#Homemade methods and classes (includes astropy tools, not written by me)\n",
    "import methods_and_classes.soam_class as soam\n",
    "from methods_and_classes.simbad_alias_search import online_alias_search\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75a5e753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOAM Started ---------------- \n",
      "SOAM Cleaning Method Test -- \n",
      "original test string: \"   #][!,@ ^&*NGc224-.99+9abc. ...   \"\n",
      "cleaned test string: \"ngc 224 99 9 abc\"\n"
     ]
    }
   ],
   "source": [
    "#My new SOAM\n",
    "my_soam = soam.Soam() #initiating my SOAM\n",
    "\n",
    "#STARTING with seed associations scraped from the web (wikipedia lists and the like...)\n",
    "file_path = \".\\data\\AstroCatelogues.xlsm\" #the file containing all the \"seed\" associations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62795db2",
   "metadata": {},
   "source": [
    "## LOADING SEED ASSOCIATIONS INTO THE SOAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d9e946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 names / aliases mapped across 109 objects.\n"
     ]
    }
   ],
   "source": [
    "#Start Loading in various Catalogs with some associations\n",
    "sheet_name = \"Messier\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "select_cols = [\"Messier number\",\"NGC/IC number\",\"Common name\"]\n",
    "listed_values = df[select_cols].values.tolist()\n",
    "associations = [[item.replace('\\xa0', '') for item in sublist if item != '–'] for sublist in listed_values]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c36bba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 names / aliases mapped across 218 objects.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "sheet_name = \"Caldwell\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "select_cols = [\"Caldwell number\",\"NGC number\"]\n",
    "listed_values = df[select_cols].values.tolist()\n",
    "associations = [[item.replace('\\xa0', '') for item in sublist if item != '-' ] for sublist in listed_values]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "704aad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693 names / aliases mapped across 411 objects.\n"
     ]
    }
   ],
   "source": [
    "sheet_name = \"Planets_and_DwarfPlanets\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "# we need to change \"(45) Eugenia I (Petit-Prince)\" into ['', '45',' Eugenia I ', 'Petit-Prince', '' ]\n",
    "associations = list(df[\"Body\"].str.split(r'[\\(\\)]'))\n",
    "#now we need to change ['', '45',' Eugenia I ', 'Petit-Prince', '' ] into ['Eugenia I', 'Petit-Prince']\n",
    "associations = [[word.replace('\\xa0', ' ').rstrip().lstrip() for word in sublist if (word != '') and not (word.isdigit())] for sublist in associations]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a15a09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349 names / aliases mapped across 1077 objects.\n"
     ]
    }
   ],
   "source": [
    "sheet_name = \"Galactic Wolf Rayet\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "df['WR'] = \"WR \" + (df['WR#'])\n",
    "select_cols = [\"WR\",\"HD\",\"Alias1\",\"Alias2\",\"Alias3\"]\n",
    "listed_values = df[select_cols].astype(str).values.tolist()\n",
    "#cleaning out the bad characters, removing 'nan's, and stripping out right hand spaces\n",
    "listed_values = [[word.replace('\\xa0', ' ').rstrip() for word in sublist if word != 'nan'] for sublist in listed_values]\n",
    "#getting rid of the last value (not a real association)\n",
    "associations = listed_values[:-1]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "879152aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479 names / aliases mapped across 1101 objects.\n"
     ]
    }
   ],
   "source": [
    "sheet_name = \"Sharpless\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "\n",
    "#part 1 of the sharpless associations\n",
    "c = \"Names and designations\"\n",
    "designations_list = df[c].str.split(r',').values.tolist()\n",
    "associations_1 = [[word.replace('\\xa0', '').lstrip() for word in sublist] for sublist in designations_list]    \n",
    "\n",
    "#part 2 of the sharpless associations\n",
    "df[\"Sh2\"] = \"Sh2 \" + df['Sh2 No.'].astype(str)\n",
    "select_cols = [\"Sh2\", \"A common name\"]\n",
    "associations_2 = df[select_cols].astype(str).values.tolist()\n",
    "\n",
    "associations = [set(associations_1[i] + associations_2[i]) for i in range(len(associations_1))]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9ddf294",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490 names / aliases mapped across 1106 objects.\n"
     ]
    }
   ],
   "source": [
    "sheet_name = \"Gum\"\n",
    "df = pd.read_excel(file_path,sheet_name)\n",
    "\n",
    "#part 1 of the gum associations\n",
    "c = \"Names & Designations\"\n",
    "designations_list = df[c].str.split(r',').values.tolist()\n",
    "associations_1 = [[word.replace('\\xa0', '').lstrip() for word in sublist] for sublist in designations_list]    \n",
    "\n",
    "#part 2 of the gum associations\n",
    "select_cols = [\"Gum\", \"A common name\"]\n",
    "associations_2 = df[select_cols].astype(str).values.tolist()\n",
    "\n",
    "associations = [set(associations_1[i] + associations_2[i]) for i in range(len(associations_1))]\n",
    "\n",
    "#Add the associations to the SOAM\n",
    "my_soam.add_associations(associations)\n",
    "print(my_soam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d746be",
   "metadata": {},
   "source": [
    "## LOAD SELECT CATALOG IDS FROM ASTROBIN TITLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b492097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comets and other minor bodies could be added to this alias map if desired...\n",
    "#Check out https://sbpy.readthedocs.io/en/latest/api/sbpy.data.Names.html#sbpy.data.Names\n",
    "#from sbpy.data import Names\n",
    "#... Not really sure how useful it would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a615804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START LOADING---\n",
      "--- END LOADING---\n",
      "--- START CLEANING---\n",
      "1837 Null Rows Removed\n",
      "2394 Null Title Rows Removed\n",
      "---CLEANING DONE---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- START LOADING---\")\n",
    "# Data Load (provided by Salvatore from AstroBin site)\n",
    "df = pd.read_csv(\".\\\\data\\\\astrobin_titles_to_subject_types.csv\") #the main starting data frame\n",
    "print(\"--- END LOADING---\")\n",
    "print(\"--- START CLEANING---\")\n",
    "#HOUSE KEEPING -- Casting 'subject_type' and 'solar_system_main_subject' as categories\n",
    "cats = ['subject_type','solar_system_main_subject']\n",
    "df[cats] = df[cats].astype('category')\n",
    "\n",
    "#CLEANING -- remove all null rows\n",
    "before = df.shape[0] #row count before\n",
    "df = df.dropna(how='all') #droping only the rows with 'na' across ALL columns\n",
    "after = df.shape[0] #row count after\n",
    "\n",
    "print(str(before-after)+\" Null Rows Removed\")\n",
    "\n",
    "#CLEANING -- remove all null rows\n",
    "before = df.shape[0] #row count before\n",
    "df = df.dropna(subset=['title']) #droping only the rows with 'na' ONLY in title column\n",
    "df = df[~df['title'].str.isspace()] #droping only the rows with '  ' (white space) as title\n",
    "df = df[df['title']!=''] #droping only the rows with '' (dead space) as title\n",
    "after = df.shape[0] #row count after\n",
    "\n",
    "print(str(before-after)+\" Null Title Rows Removed\")\n",
    "\n",
    "#CLEANING the unwanted words out of the string column (use all lower case)\n",
    "unwanted_words = ['the', 'and', \"in\", \"of\"] #there are other words like \" on \" \n",
    "# ...but those may be more indicative of the category they are found in... \n",
    "#... (like \" on \" being a very common word in GEAR)...\n",
    "#... (or like \" at \" being a very common word in STAR_TRAILS)\n",
    "\n",
    "#CLEANING some other language words out\n",
    "other_unwanted_words = [\n",
    "    'du', 'la', 'de', 'le'\n",
    "]\n",
    "#If you want to add this extra cleaning step for other languages, leave this line\n",
    "unwanted_words = unwanted_words+other_unwanted_words\n",
    "\n",
    "# Regular expression pattern to match any unwanted word\n",
    "pattern = r'\\b(?:{})\\b'.format('|'.join(unwanted_words))\n",
    "# Remove unwanted words using the pattern with str.replace() and put cleaned strings their own col\n",
    "df['cleaned_title'] = df['title'].str.lower().replace(pattern, '', regex=True)\n",
    "print(\"---CLEANING DONE---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7678b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a vectorizer with ngram min/max set of uni and bi ngrams\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2)) \n",
    "\n",
    "ngrams = vectorizer.fit_transform(df.cleaned_title) #vectorize that string column\n",
    "\n",
    "#collect frequencies of counts from vectorized values\n",
    "ngram_freq = pd.DataFrame(ngrams.sum(axis=0), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "all_ngrams = ngram_freq.T.sort_values(by=0, ascending=False) #return all n ngrams\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c68c1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATION -- review cleaned title data for most popular catalog IDs\n",
    "\n",
    "## -- first, a dict to help recognize catlog IDs\n",
    "catalog_tags = {\n",
    "    'messier':['messier','m'],\n",
    "    'ic':['ic','index'],\n",
    "    'ngc':['ngc','new general'],\n",
    "    'hd':['hd','henry draper'],\n",
    "    'caldwell':['caldwell','c'],\n",
    "    'gum':['gum'],\n",
    "    'sh2':['sharpless','sh2'],\n",
    "    'rcw':['rcw']\n",
    "}\n",
    "\n",
    "## -- then, lets merge all tags into a check list\n",
    "checklist = [x for every_x in catalog_tags.values() for x in every_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7869e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- then, we understand the following and apply it to a regex search of each 1 and 2 ngram\n",
    "#  -- -- each tag should be followed by a series of number characters [0-9]\n",
    "#  -- -- each series of number characters of an objects ID will end with a non-numeric character.                 \n",
    "#  -- -- this will not capture multiple IDs that were 'cleverly' written like:\n",
    "#        \"Messier 24,16,17 and m23\" (our regex will only catch \"Messier 24\" and \"m23\")\n",
    "\n",
    "#HELPER METHOD -- regex for catalog patterns in titles, default values are only for testing purposes\n",
    "def id_search(tags = ['messier','m'], s = \"This string 'messier 24,16, 17 and m23, m-05' will not return 16 nor 17\"):      \n",
    "    #ASSUMES tags and s are cleaned accordingly (cap sensitive)\n",
    "    # Create a regular expression pattern with capturing groups for the keyword and numeric characters\n",
    "    pattern = r'(' + '|'.join(re.escape(keyword) for keyword in tags) + r')\\D*(\\d+)'\n",
    "    # Use re.findall to find all matching substrings\n",
    "    matches = re.findall(pattern, s)\n",
    "    # return the matched keyword and numeric characters\n",
    "    return([' '.join(match) for match in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4695a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORK -- pull IDs from title data\n",
    "title_ids = [id_search(tags = checklist, s = x) for x in df.cleaned_title] #collect id_search results, includes empty '[]' and nested '[A, B]'      \n",
    "title_ids = [x for every_x in title_ids for x in every_x] #remove empties  and un-nest values, still contains duplicates\n",
    "title_ids = set(title_ids) #removes duplicates and makes title ids a set\n",
    "#title_ids #we now have a set of IDS listed from titles data (some ids are still nonsence, like 'm 20190716')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7c2c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITLE ASSISTED SELECTIONS\n",
    "ngc_selections = [x for x in title_ids if 'ngc' in x]+[x for x in title_ids if 'new' in x]\n",
    "ic_selections = [x for x in title_ids if 'ic' in x]+[x for x in title_ids if 'index' in x]\n",
    "hd_selections = [x for x in title_ids if 'hd' in x]+[x for x in title_ids if 'draper' in x]\n",
    "#... these selections still need to be cleaned\", but the rules of these cleanings may differ between the catalog selection types \n",
    "#simple cleaning -- filtering out the values that are obviously too long to be real ids\n",
    "ngc_selections = [x for x in ngc_selections if len(x)<=8] \n",
    "ic_selections = [x for x in ic_selections if len(x)<=7]\n",
    "hd_selections = [x for x in hd_selections if len(x)<=9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f771d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "astrobin_select_ids = ngc_selections+ic_selections+hd_selections\n",
    "\n",
    "## DO NOT UNCOMMENT unless you need to... will run for hours... --------\n",
    "#search_return = online_alias_search(astrobin_select_ids)\n",
    "#bulk_associations = [x[\"found_associations\"] for x in search_return[\"associations\"]]\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "#INSTEAD...\n",
    "# laod a json file that I already took the time to pull from \n",
    "file_name = '.\\\\data\\\\ngc_ic_hd_associations_data.json'\n",
    "# Read the JSON file and convert it back to a list of lists\n",
    "bulk_associations = []\n",
    "with open(file_name, 'r') as file:\n",
    "    bulk_associations = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22ab5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_bulk = []\n",
    "\n",
    "#clean out certain two-character leads in associations\n",
    "bad_leads = [\n",
    "    \"M \", #we already have Messier IDs in the SOAM from above steps\n",
    "    \"Z \" #not a common ID\n",
    "]\n",
    "for association in bulk_associations:\n",
    "    cleaned_bulk.append([name for name in association if not name[:2] in bad_leads])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "324ad96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all ids that are just numerical characters\n",
    "cleaned_bulk = [[item for item in inner_list if not item.isnumeric()] for inner_list in cleaned_bulk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfaddd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all ids that are shorter than 3 characters long\n",
    "cleaned_bulk = [[item for item in inner_list if len(item)>=3] for inner_list in cleaned_bulk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "664a8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all ids that start with a numeric character\n",
    "cleaned_bulk = [[item for item in inner_list if not item[0].isnumeric()] for inner_list in cleaned_bulk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef2b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the excessivly long \"Gaia \" ids\n",
    "cleaned_bulk = [[item for item in inner_list if item[:5] != 'Gaia '] for inner_list in cleaned_bulk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3b3e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the \"HIDDEN \" tag from any id that has this...\n",
    "cleaned_bulk = [[item.replace('HIDDEN ', '') for item in inner_list] for inner_list in cleaned_bulk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9de48ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LEDA 69401', 'MAPS-PP O-778-826778', 'NGC 7345', 'UGC 12130'],\n",
       " ['C 0746-211', 'NGC 2455', 'MWSC 1350'],\n",
       " ['GLEAM J202507-244839',\n",
       "  'HIPASS J2025-24',\n",
       "  'SINGG HIPASS J2025-24',\n",
       "  'AGC 33399',\n",
       "  'PMN J2025-2448',\n",
       "  'NVSS J202507-244832',\n",
       "  'ESO 528-3',\n",
       "  'ESO-LV 528-0030',\n",
       "  'IRAS 20221-2458',\n",
       "  'IRAS F20221-2458',\n",
       "  'LEDA 64650',\n",
       "  'MCG-04-48-006',\n",
       "  'NGC 6907',\n",
       "  'UGCA 418',\n",
       "  'PSCz Q20221-2458'],\n",
       " ['Ark 582', 'GIN 675', 'LEDA 71011', 'NGC 7598', 'ACO 2572 B'],\n",
       " ['Ka 111', 'LEDA 59344', 'NGC 6285', 'PRC C-51', 'APG 293A', 'PN VV 527'],\n",
       " ['LEDA 53933', 'NGC 5866', 'UGC 9723'],\n",
       " ['TIC 150971494', 'UCAC2 39602310', 'UCAC3 225-1775', 'NGC 91'],\n",
       " ['Ark 1', 'LEDA 565', 'NGC 3', 'UGC 58'],\n",
       " ['CGMW 5-6149', 'LEDA 62178', 'NGC 6674', 'TC 868', 'UGC 11308'],\n",
       " ['LH 80', 'NGC 2028', 'OGLE-CL LMC 594', 'ESO 56-152']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_bulk[0:10] #example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52398337",
   "metadata": {},
   "source": [
    "## ADDING BULK ASSOCIATIONS TO THE SOAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "281f211b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490 names / aliases mapped across 1106 objects.\n",
      "25631 names / aliases mapped across 4264 objects.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "As a brief reminder of where we are at, see the below example:\n",
    "\n",
    "EXAMPLE:\n",
    "        t = Soam()\n",
    "        cleaned_bulk = [[\"a\",\"b\",\"c\"],[\"c\",\"d\"],[\"e\",\"f\"] # snippits of associations (not all fully associated yet)\n",
    "        t.add_associations(cleaned_bulk)\n",
    "        print(t.all_names())\n",
    "        print(t.all_aliases())\n",
    "        print(f\"'c' is also known as {t.get_aliases('c')}\")\n",
    "    RETURNS:\n",
    "        {'d': 0, 'b': 0, 'a': 0, 'c': 0, 'f': 1, 'e': 1}\n",
    "        {0: {'d', 'b', 'a', 'c'}, 1: {'f', 'e'}}\n",
    "        'c' is also known as {'d', 'b', 'a', 'c'}\n",
    "\"\"\"\n",
    "print(my_soam)\n",
    "my_soam.add_associations(cleaned_bulk)\n",
    "print(my_soam)\n",
    "my_soam.export_soam(file_name=\"soam_cleaned_bulk_export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b3203",
   "metadata": {},
   "source": [
    "# A Word of Warning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afc04291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'m 38', 'mwsc 0557', 'ngc 1912', 'starfish cluster', 'theia 874'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unfortunately there were some pockets where the associations fed into the SOAM were incorrect. I've correct what I could, but other pockets may still be in here, unknown to me:\n",
    "#EXAMPLE:\n",
    "print(\"Fixed Example:\")\n",
    "my_soam.get_aliases(\"m38\")\n",
    "# used to return:\n",
    "#  {'butterfly cluster',\n",
    "#   ...\n",
    "#   'm 38',\n",
    "#   'm 43',\n",
    "#   'm 6',\n",
    "#   \"mairan's nebula\",\n",
    "#   ...\n",
    "#   'starfish cluster',\n",
    "#   ...}\n",
    "\n",
    "# we know that those are three distinct objects, because there were 3 Messier catalogue IDs in the same space object alias bin (M38, M43, and M6) \n",
    "# And those ID-ed space objects are not even close in proximaty to eachother in the sky...\n",
    "\n",
    "#HOW DOES THIS HAPPEN?\n",
    "# -- These types of \"over-agressive\" association between distinctly different\n",
    "#    objects appears to stem from SIMBAD\n",
    "# -- Recall there was a SOAM building step where SIMBAD was used to \n",
    "#    query all associated ids and names given either an ngc, ic, or hd catalog id\n",
    "# -- It appears that SIMBAD has some bad data here...\n",
    "\n",
    "#EXAMPLE:\n",
    "#an online simbad search (as performed through my old method)\n",
    "#online_alias_search([\"NGC 6405\"])\n",
    "#returns\n",
    "#'associations': [{'searched': 'NGC 6405',\n",
    "#   'found_associations': [\n",
    "#     ...\n",
    "#    'M 6',\n",
    "#    'NGC 6405',\n",
    "#    'M 38', #this should not be here\n",
    "#    'M 43', #this should not be here\n",
    "#     ...\n",
    "#    'Butterfly Cluster']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bebe773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I fixed the above issue of having possible mis-associated Messier objects by \n",
    "#  cleaning the JSON file from the initial SIMBAD query\n",
    "cleaned_bulk = []\n",
    "for association in bulk_associations:\n",
    "    cleaned_bulk.append([name for name in association if not name.startswith(\"M \")])\n",
    "    \n",
    "#... But there could be other issues similar to the Messier object error... \n",
    "\n",
    "# Instead of re-running the SIMBAD query, I chose to filter out all \"M \" words from the \n",
    "# JSON file instead (as these were already collected in the Seed portion of building the SOAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fd545",
   "metadata": {},
   "source": [
    "## Other Cleaning Tasks for the SOAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b10b4da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(cleaned_soam)\n",
    "\n",
    "#pulling names that contain \"and\" or \"or\" (we will likely need to address the conjunction \"or\")\n",
    "[x for x in cleaned_soam.all_names() if (len(x.split()) > 3) and ((\"or\" in x) or (\"and\" in x))]\n",
    "\n",
    "conjuncted_names = ['omega swan horseshoe lobster or checkmark nebula',\n",
    " 'beehive cluster or praesepe',\n",
    " 'pleiades seven sisters or subaru',\n",
    " 'cetus a or squid galaxy',\n",
    " 'virgo a or smoking gun galaxy',\n",
    " \"crocodile eye or cat's eye galaxy\"]\n",
    "\n",
    "[x.split(\" or \") for x in conjuncted_names]\n",
    "\n",
    "associations = split_names = [\n",
    "    ['omega swan horseshoe lobster or checkmark nebula','omega nebula','swan nebula','horseshoe nebula','lobster nebula', 'checkmark nebula'],\n",
    "    [ 'beehive cluster or praesepe','beehive cluster', 'praesepe'],\n",
    "    ['pleiades seven sisters or subaru','pleiades','seven sisters', 'subaru'],\n",
    "    ['cetus a or squid galaxy', 'cetus a galaxy', 'squid galaxy'],\n",
    "    ['virgo a or smoking gun galaxy', 'virgo a galaxy', 'smoking gun galaxy'],\n",
    "    [ \"crocodile eye or cat's eye galaxy\",'crocodile eye galaxy', \"cat's eye galaxy\"]]\n",
    "\n",
    "cleaned_soam.add_associations(associations)\n",
    "print(cleaned_soam)\n",
    "\n",
    "#uncomment only if you really want to redo your cleaned export\n",
    "#cleaned_soam.export_soam(file_name=\"soam_cleaned_bulk_export\") \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
